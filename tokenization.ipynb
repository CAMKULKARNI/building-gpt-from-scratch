{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCPkwBDXhz2D"
      },
      "source": [
        "Designed as a walkthrough with this [YouTube video](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkAPaUCXOhvW"
      },
      "source": [
        "# Tokenization :(\n",
        "\n",
        "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
        "\n",
        "- Why can't LLM spell words? **Tokenization**.\n",
        "- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.\n",
        "- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization**.\n",
        "- Why is LLM bad at simple arithmetic? **Tokenization**.\n",
        "- Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization**.\n",
        "- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? **Tokenization**.\n",
        "- What is this weird warning I get about a \"trailing whitespace\"? **Tokenization**.\n",
        "- Why the LLM break if I ask it about \"SolidGoldMagikarp\"? **Tokenization**.\n",
        "- Why should I prefer to use YAML over JSON with LLMs? **Tokenization**.\n",
        "- Why is LLM not actually end-to-end language modeling? **Tokenization**.\n",
        "- What is the real root of suffering? **Tokenization**.\n",
        "\n",
        "---\n",
        "\n",
        "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
        "\n",
        "Example string:\n",
        "\n",
        "```\n",
        "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
        "\n",
        "127 + 677 = 804\n",
        "1275 + 6773 = 8041\n",
        "\n",
        "Egg.\n",
        "I have an Egg.\n",
        "egg.\n",
        "EGG.\n",
        "\n",
        "만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\n",
        "\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Much glory awaits someone who can delete the need for tokenization. But meanwhile, let's learn about it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PeU63eDYOhve",
        "outputId": "cb199e80-9d92-4fe8-8179-2cf0accb3db7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'안녕하세요 👋 (hello in Korean!)'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"안녕하세요 👋 (hello in Korean!)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_paQxu7EOhvg",
        "outputId": "2eddf9b6-0602-42ee-e2c7-b7607ba99816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[50504,\n",
              " 45397,\n",
              " 54616,\n",
              " 49464,\n",
              " 50836,\n",
              " 32,\n",
              " 128075,\n",
              " 32,\n",
              " 40,\n",
              " 104,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 32,\n",
              " 75,\n",
              " 111,\n",
              " 114,\n",
              " 101,\n",
              " 97,\n",
              " 110,\n",
              " 33,\n",
              " 41]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[ord(x)for x in \"안녕하세요 👋 (hello in Korean!)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDQBEVPTOhvg",
        "outputId": "29ce5add-8bab-4329-ba4c-343dfd960590",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[236,\n",
              " 149,\n",
              " 136,\n",
              " 235,\n",
              " 133,\n",
              " 149,\n",
              " 237,\n",
              " 149,\n",
              " 152,\n",
              " 236,\n",
              " 132,\n",
              " 184,\n",
              " 236,\n",
              " 154,\n",
              " 148,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 32,\n",
              " 40,\n",
              " 104,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 32,\n",
              " 75,\n",
              " 111,\n",
              " 114,\n",
              " 101,\n",
              " 97,\n",
              " 110,\n",
              " 33,\n",
              " 41]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, [32])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ord(\" \"), list(\" \".encode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wm3T6JYOhvg",
        "outputId": "7e624d8c-f5aa-4acc-f195-c6f7a8ed4940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
            "length: 533\n",
            "---\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n"
          ]
        }
      ],
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128516, [240, 159, 152, 132])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ord(\"😄\"), list(map(int, \"😄\".encode(\"utf-8\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
          ]
        }
      ],
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(tokens)\n",
        "# print(stats)\n",
        "\n",
        "print(sorted(((v, k) for k, v in stats.items()), reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5, 6, 100, 9, 1]\n"
          ]
        }
      ],
      "source": [
        "def merge(current_tokens: list, pair: tuple, idx: int) -> list:\n",
        "    i = 0\n",
        "    new_tokens = []\n",
        "    while i < len(current_tokens):\n",
        "        if i < len(current_tokens) - 1 and current_tokens[i] == pair[0] and current_tokens[i + 1] == pair[1]:\n",
        "            new_tokens.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(current_tokens[i])\n",
        "            i += 1\n",
        "    \n",
        "    return new_tokens\n",
        "\n",
        "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 100))   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "596\n"
          ]
        }
      ],
      "source": [
        "top_pair = max(stats, key=stats.get)\n",
        "tokens2 = merge(tokens, top_pair, 256)\n",
        "print(len(tokens2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (8, (116, 104)), (8, (32, 97)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (101, 114)), (6, (32, 116)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (104, 256)), (5, (44, 32)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (109, 101)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 105)), (4, (32, 102)), (3, (256, 116)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (100, 101)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 119)), (3, (32, 115)), (3, (32, 111)), (3, (32, 100)), (3, (32, 98)), (3, (32, 85)), (2, (256, 115)), (2, (256, 105)), (2, (256, 97)), (2, (128, 153)), (2, (121, 32)), (2, (119, 256)), (2, (119, 104)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 256)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 256)), (2, (109, 109)), (2, (108, 256)), (2, (108, 105)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 256)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 256)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 112)), (2, (32, 110)), (2, (32, 109)), (1, (256, 119)), (1, (256, 118)), (1, (256, 117)), (1, (256, 112)), (1, (256, 111)), (1, (256, 109)), (1, (256, 104)), (1, (256, 99)), (1, (256, 85)), (1, (256, 83)), (1, (256, 40)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 101)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 101)), (1, (108, 100)), (1, (108, 97)), (1, (107, 256)), (1, (107, 110)), (1, (107, 101)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (104, 101)), (1, (103, 256)), (1, (103, 115)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 256)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 108)), (1, (32, 107)), (1, (32, 101)), (1, (32, 99)), (1, (32, 87)), (1, (32, 84)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51))]\n"
          ]
        }
      ],
      "source": [
        "stats = get_stats(tokens2)\n",
        "print(sorted(((v, k) for k, v in stats.items()), reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = None\n",
        "with open(\"text.txt\", \"r\") as file:\n",
        "    text = file.readlines()[0]\n",
        "tokens = text.encode(\"utf-8\")\n",
        "tokens = list(map(int, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merging (101, 32) into new token 256\n",
            "merging (105, 110) into new token 257\n",
            "merging (115, 32) into new token 258\n",
            "merging (116, 104) into new token 259\n",
            "merging (101, 114) into new token 260\n",
            "merging (99, 111) into new token 261\n",
            "merging (116, 32) into new token 262\n",
            "merging (226, 128) into new token 263\n",
            "merging (44, 32) into new token 264\n",
            "merging (97, 110) into new token 265\n",
            "merging (111, 114) into new token 266\n",
            "merging (100, 32) into new token 267\n",
            "merging (97, 114) into new token 268\n",
            "merging (101, 110) into new token 269\n",
            "merging (257, 103) into new token 270\n",
            "merging (261, 100) into new token 271\n",
            "merging (121, 32) into new token 272\n",
            "merging (46, 32) into new token 273\n",
            "merging (97, 108) into new token 274\n",
            "merging (259, 256) into new token 275\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 276\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens)\n",
        "\n",
        "merges = {}\n",
        "for i in range(num_merges):\n",
        "    stats = get_stats(ids)\n",
        "    pair = max(stats, key=stats.get)\n",
        "    idx = 256 + i\n",
        "    ids = merge(ids, pair, idx)\n",
        "    print(f\"merging {pair} into new token {idx}\")\n",
        "    merges[pair] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "def decode(ids: list[int]):\n",
        "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "    text = tokens.decode(\"utf-8\", errors='replace')\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n"
          ]
        }
      ],
      "source": [
        "def encode(text: str):\n",
        "    encoding = text.encode(\"utf-8\")\n",
        "    tokens = list(map(int, encoding))\n",
        "\n",
        "    while len(tokens) >= 2:\n",
        "        stats = get_stats(tokens)\n",
        "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "        if pair not in merges: break\n",
        "\n",
        "        idx = merges[pair]\n",
        "        tokens = merge(tokens, pair, idx)\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "print(encode(\"hello world!\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

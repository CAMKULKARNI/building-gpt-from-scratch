{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\" \".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [stoi[ch] for ch in x] # encoder: take a string, output a list of integers\n",
    "decode = lambda x: \"\".join([itos[i] for i in x]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "Hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hii there\"))\n",
    "print(decode([20, 47, 47, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:10:02.391237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750907402.737027    1060 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750907402.831293    1060 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750907403.585849    1060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750907403.585906    1060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750907403.585912    1060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750907403.585916    1060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-26 03:10:03.696452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int64'>\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1 39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39\n",
      " 58 46 43 56  1 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47\n",
      " 57 46 12  0  0 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53\n",
      " 50 60 43 42  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47\n",
      " 56 57 58  6  1 63 53 59  1 49 52 53 61  1 15 39 47 59 57  1 25 39 56 41\n",
      " 47 59 57  1 47 57  1 41 46 47 43 44  1 43 52 43 51 63  1 58 53  1 58 46\n",
      " 43  1 54 43 53 54 50 43  8  0  0 13 50 50 10  0 35 43  1 49 52 53 61  5\n",
      " 58  6  1 61 43  1 49 52 53 61  5 58  8  0  0 18 47 56 57 58  1 15 47 58\n",
      " 47 64 43 52 10  0 24 43 58  1 59 57  1 49 47 50 50  1 46 47 51  6  1 39\n",
      " 52 42  1 61 43  5 50 50  1 46 39 60 43  1 41 53 56 52  1 39 58  1 53 59\n",
      " 56  1 53 61 52  1 54 56 47 41 43  8  0 21 57  5 58  1 39  1 60 43 56 42\n",
      " 47 41 58 12  0  0 13 50 50 10  0 26 53  1 51 53 56 43  1 58 39 50 49 47\n",
      " 52 45  1 53 52  5 58 11  1 50 43 58  1 47 58  1 40 43  1 42 53 52 43 10\n",
      "  1 39 61 39 63  6  1 39 61 39 63  2  0  0 31 43 41 53 52 42  1 15 47 58\n",
      " 47 64 43 52 10  0 27 52 43  1 61 53 56 42  6  1 45 53 53 42  1 41 47 58\n",
      " 47 64 43 52 57  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 35\n",
      " 43  1 39 56 43  1 39 41 41 53 59 52 58 43 42  1 54 53 53 56  1 41 47 58\n",
      " 47 64 43 52 57  6  1 58 46 43  1 54 39 58 56 47 41 47 39 52 57  1 45 53\n",
      " 53 42  8  0 35 46 39 58  1 39 59 58 46 53 56 47 58 63  1 57 59 56 44 43\n",
      " 47 58 57  1 53 52  1 61 53 59 50 42  1 56 43 50 47 43 60 43  1 59 57 10\n",
      "  1 47 44  1 58 46 43 63  0 61 53 59 50 42  1 63 47 43 50 42  1 59 57  1\n",
      " 40 59 58  1 58 46 43  1 57 59 54 43 56 44 50 59 47 58 63  6  1 61 46 47\n",
      " 50 43  1 47 58  1 61 43 56 43  0 61 46 53 50 43 57 53 51 43  6  1 61 43\n",
      "  1 51 47 45 46 58  1 45 59 43 57 57  1 58 46 43 63  1 56 43 50 47 43 60\n",
      " 43 42  1 59 57  1 46 59 51 39 52 43 50 63 11  0 40 59 58  1 58 46 43 63\n",
      "  1 58 46 47 52 49  1 61 43  1 39 56 43  1 58 53 53  1 42 43 39 56 10  1\n",
      " 58 46 43  1 50 43 39 52 52 43 57 57  1 58 46 39 58  0 39 44 44 50 47 41\n",
      " 58 57  1 59 57  6  1 58 46 43  1 53 40 48 43 41 58  1 53 44  1 53 59 56\n",
      "  1 51 47 57 43 56 63  6  1 47 57  1 39 57  1 39 52  0 47 52 60 43 52 58\n",
      " 53 56 63  1 58 53  1 54 39 56 58 47 41 59 50 39 56 47 57 43  1 58 46 43\n",
      " 47 56  1 39 40 59 52 42 39 52 41 43 11  1 53 59 56  0 57 59 44 44 43 56\n",
      " 39 52 41 43  1 47 57  1 39  1 45 39 47 52  1 58 53  1 58 46 43 51  1 24\n",
      " 43 58  1 59 57  1 56 43 60 43 52 45 43  1 58 46 47 57  1 61 47 58 46  0\n",
      " 53 59 56  1 54 47 49 43 57  6  1 43 56 43  1 61 43  1 40 43 41 53 51 43\n",
      "  1 56 39 49 43 57 10  1 44 53 56  1 58 46 43  1 45 53 42 57  1 49 52 53\n",
      " 61  1 21  0 57 54 43 39 49  1 58 46 47 57  1 47 52  1 46 59 52 45 43 56\n",
      "  1 44 53 56  1 40 56 43 39 42  6  1 52 53 58  1 47 52  1 58 46 47 56 57\n",
      " 58  1 44 53 56  1 56 43 60 43 52 45 43  8  0  0], shape=(1000,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750907417.254251    1060 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2242 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Let's convert the entire encoded dataset into a tensorflow representation of a tensor\n",
    "data = tf.convert_to_tensor(encode(text), dtype=tf.int64)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the dataset into training and validation sets\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1003854]), TensorShape([111540]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([18 47 56 57 58  1 15 47 58], shape=(9,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "print(train_data[:block_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18] Target: 47 \n",
      "String: F -> i \n",
      "\n",
      "\n",
      "Context: [18 47] Target: 56 \n",
      "String: Fi -> r \n",
      "\n",
      "\n",
      "Context: [18 47 56] Target: 57 \n",
      "String: Fir -> s \n",
      "\n",
      "\n",
      "Context: [18 47 56 57] Target: 58 \n",
      "String: Firs -> t \n",
      "\n",
      "\n",
      "Context: [18 47 56 57 58] Target: 1 \n",
      "String: First ->   \n",
      "\n",
      "\n",
      "Context: [18 47 56 57 58  1] Target: 15 \n",
      "String: First  -> C \n",
      "\n",
      "\n",
      "Context: [18 47 56 57 58  1 15] Target: 47 \n",
      "String: First C -> i \n",
      "\n",
      "\n",
      "Context: [18 47 56 57 58  1 15 47] Target: 58 \n",
      "String: First Ci -> t \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    \n",
    "    print(\"Context:\", context.numpy(), \"Target:\", target.numpy(), \"\\nString:\", decode(context.numpy()), \"->\", decode([target.numpy()]), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tf.Tensor(\n",
      "[[ 1 39  1 57 43 60 43 56]\n",
      " [ 0 19 30 17 37 10  0 26]\n",
      " [51 51 43 56  1 40 43 39]\n",
      " [13 50 58 46 53 59 45 46]], shape=(4, 8), dtype=int64)\n",
      "(4, 8)\n",
      "y: \n",
      "tf.Tensor(\n",
      "[[39  1 57 43 60 43 56 39]\n",
      " [19 30 17 37 10  0 26 53]\n",
      " [51 43 56  1 40 43 39 59]\n",
      " [50 58 46 53 59 45 46  1]], shape=(4, 8), dtype=int64)\n",
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "def get_batch(spilt, batch_size=4):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if spilt == \"train\" else val_data\n",
    "\n",
    "    ix = tf.random.uniform((batch_size,), maxval=data.shape[0] - block_size, dtype=tf.int64)\n",
    "    x = tf.stack([data[i:i + block_size] for i in ix])\n",
    "    y = tf.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "\n",
    "print(\"x: \")\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(\"y: \")\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(keras.Model):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        # Each token directly reads off the the logits for the next token from a lookup table\n",
    "        self.embedding = keras.layers.Embedding(input_dim=vocab_size, output_dim=vocab_size)\n",
    "    \n",
    "    def call(self, idx, targets=None) -> tf.Tensor:\n",
    "        # idx and targets are both tensors of shape [batch_size, block_size]\n",
    "        logits = self.embedding(idx) # shape [batch_size, block_size, vocab_size]\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            # I am doubting that this might cause problem in the accuracy\n",
    "            # If there is a problem in accuracy, this is the first place I would look at\n",
    "            loss = keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a tensor of shape [batch_size, block_size]\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            \n",
    "            # focus only on the last token\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # apply softmax to convert logits to probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "            # sample from the probability distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1) # shape [batch_size, 1]\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, idx_next], axis=-1) # shape [batch_size, block_size + 1]\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.1698456, shape=(), dtype=float32)\n",
      "(4, 8, 65)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(x, y)\n",
    "print(loss)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 44, 43, 34, 29, 28, 19, 42, 30, 42, 53, 36, 46, 16, 18, 12, 51, 43, 35, 24, 36, 14, 28, 1, 21, 26, 27, 17, 56, 59, 12, 24, 63, 9, 23, 2, 54, 45, 13, 4, 2, 50, 19, 18, 18, 6, 6, 44, 45, 33, 15, 14, 23, 24, 50, 60, 43, 25, 59, 22, 2, 47, 2, 22, 48, 58, 51, 41, 0, 58, 41, 18, 6, 55, 31, 20, 14, 4, 4, 5, 63, 1, 0, 57, 42, 36, 44, 16, 10, 46, 37, 50, 63, 45, 11, 0, 55, 34, 41, 3, 18]\n",
      "\n",
      "feVQPGdRdoXhDF?meWLXBP INOEru?Ly3K!pgA&!lGFF,,fgUCBKLlveMuJ!i!Jjtmc\n",
      "tcF,qSHB&&'y \n",
      "sdXfD:hYlyg;\n",
      "qVc$F\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the random output of the model\n",
    "idx = tf.zeros((1, 1), dtype=tf.int64)\n",
    "output = model.generate(idx, max_new_tokens=100)[0].numpy().tolist()\n",
    "print(output)\n",
    "print(decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.50it/s, loss=3.94]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.9363196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's now train the model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "bar = tqdm(range(100))\n",
    "batch_size = 32\n",
    "for steps in bar:\n",
    "    # get batch\n",
    "    x, y = get_batch(\"train\", batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    # get the gradients\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "    # apply the gradients\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # update the progress bar\n",
    "    bar.set_postfix(loss=loss.numpy())\n",
    "\n",
    "print(\"Loss: \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "# consider the toy example\n",
    "\n",
    "B, T, C = 4, 8, 2 # batch size, time, channels\n",
    "x = tf.random.uniform((B, T, C), dtype=tf.float32) # random input\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "# Version 1: Brute force\n",
    "\n",
    "# We want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = np.zeros((B, T, C)) # initialize the output\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow[b, t] = tf.reduce_mean(x[b, :t + 1], axis=0).numpy() # mean over the time dimension\n",
    "\n",
    "xbow = tf.convert_to_tensor(xbow, dtype=tf.float32) # convert to tensor\n",
    "print(xbow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       "array([[0.34661818, 0.75111485],\n",
       "       [0.10576785, 0.19518006],\n",
       "       [0.29496217, 0.80898356],\n",
       "       [0.6927602 , 0.16236067],\n",
       "       [0.92392874, 0.8549218 ],\n",
       "       [0.8835192 , 0.7477288 ],\n",
       "       [0.54423594, 0.01351762],\n",
       "       [0.2686417 , 0.30255306]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       "array([[0.34661818, 0.75111485],\n",
       "       [0.22619301, 0.47314745],\n",
       "       [0.24911606, 0.58509284],\n",
       "       [0.3600271 , 0.47940978],\n",
       "       [0.4728074 , 0.5545122 ],\n",
       "       [0.54125935, 0.586715  ],\n",
       "       [0.54168457, 0.50482965],\n",
       "       [0.50755423, 0.47954506]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "--\n",
      "b=\n",
      "tf.Tensor(\n",
      "[[4. 6.]\n",
      " [7. 2.]\n",
      " [7. 0.]], shape=(3, 2), dtype=float32)\n",
      "--\n",
      "c=\n",
      "tf.Tensor(\n",
      "[[18.  8.]\n",
      " [18.  8.]\n",
      " [18.  8.]], shape=(3, 2), dtype=float32)\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones((3, 3), dtype=tf.float32)\n",
    "b = tf.random.uniform((3, 2), minval=0, maxval=10, dtype=tf.int64)\n",
    "b = tf.cast(b, dtype=tf.float32)\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "a = tf.linalg.band_part(a, -1, 0) # keep the lower triangular part of a matrix\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "--\n",
      "b=\n",
      "tf.Tensor(\n",
      "[[4. 6.]\n",
      " [7. 2.]\n",
      " [7. 0.]], shape=(3, 2), dtype=float32)\n",
      "--\n",
      "c=\n",
      "tf.Tensor(\n",
      "[[ 4.  6.]\n",
      " [11.  8.]\n",
      " [18.  8.]], shape=(3, 2), dtype=float32)\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tf.Tensor(\n",
      "[[1.         0.         0.        ]\n",
      " [0.5        0.5        0.        ]\n",
      " [0.33333334 0.33333334 0.33333334]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = a / tf.reduce_sum(a, axis=-1, keepdims=True) # normalize the rows of a\n",
    "print(\"a=\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tf.Tensor(\n",
      "[[1.         0.         0.        ]\n",
      " [0.5        0.5        0.        ]\n",
      " [0.33333334 0.33333334 0.33333334]], shape=(3, 3), dtype=float32)\n",
      "--\n",
      "b=\n",
      "tf.Tensor(\n",
      "[[4. 6.]\n",
      " [7. 2.]\n",
      " [7. 0.]], shape=(3, 2), dtype=float32)\n",
      "--\n",
      "c=\n",
      "tf.Tensor(\n",
      "[[4.        6.       ]\n",
      " [5.5       4.       ]\n",
      " [6.0000005 2.6666667]], shape=(3, 2), dtype=float32)\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0.5 0.5 0 ... 0 0 0]\n",
      " [0.333333343 0.333333343 0.333333343 ... 0 0 0]\n",
      " ...\n",
      " [0.166666672 0.166666672 0.166666672 ... 0.166666672 0 0]\n",
      " [0.142857149 0.142857149 0.142857149 ... 0.142857149 0.142857149 0]\n",
      " [0.125 0.125 0.125 ... 0.125 0.125 0.125]]\n"
     ]
    }
   ],
   "source": [
    "# Version 2: Using matrix multiplication\n",
    "\n",
    "weights = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0) # create a banded matrix\n",
    "weights = weights / tf.reduce_sum(weights, axis=-1, keepdims=True) # normalize the rows of the matrix\n",
    "print(\"weights=\")\n",
    "tf.print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow2=\n",
      "[[[0.346618176 0.751114845]\n",
      "  [0.226193011 0.473147452]\n",
      "  [0.249116063 0.585092843]\n",
      "  ...\n",
      "  [0.541259408 0.586715]\n",
      "  [0.541684628 0.504829645]\n",
      "  [0.507554233 0.479545057]]\n",
      "\n",
      " [[0.326085806 0.11524272]\n",
      "  [0.224193394 0.451950431]\n",
      "  [0.470451772 0.382357478]\n",
      "  ...\n",
      "  [0.59264487 0.554859579]\n",
      "  [0.523899 0.608995914]\n",
      "  [0.528872669 0.596303]]\n",
      "\n",
      " [[0.224672079 0.717111111]\n",
      "  [0.593508124 0.745964587]\n",
      "  [0.540298343 0.588275075]\n",
      "  ...\n",
      "  [0.570862591 0.49272573]\n",
      "  [0.605455458 0.51120013]\n",
      "  [0.646199644 0.534682333]]\n",
      "\n",
      " [[0.758205891 0.515128255]\n",
      "  [0.871258378 0.373989224]\n",
      "  [0.765470862 0.474690855]\n",
      "  ...\n",
      "  [0.510176957 0.454417288]\n",
      "  [0.557033062 0.500243247]\n",
      "  [0.588110447 0.531782091]]]\n"
     ]
    }
   ],
   "source": [
    "xbow2 = weights @ x  # matrix multiplication, results in shape [B, T, C]\n",
    "print(\"xbow2=\")\n",
    "tf.print(xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.TestCase().assertAllClose(xbow, xbow2)  # check if the two tensors are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=inf>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow3=\n",
      "[[[0.346618176 0.751114845]\n",
      "  [0.226193011 0.473147452]\n",
      "  [0.249116063 0.585092843]\n",
      "  ...\n",
      "  [0.541259408 0.586715]\n",
      "  [0.541684628 0.504829645]\n",
      "  [0.507554233 0.479545057]]\n",
      "\n",
      " [[0.326085806 0.11524272]\n",
      "  [0.224193394 0.451950431]\n",
      "  [0.470451772 0.382357478]\n",
      "  ...\n",
      "  [0.59264487 0.554859579]\n",
      "  [0.523899 0.608995914]\n",
      "  [0.528872669 0.596303]]\n",
      "\n",
      " [[0.224672079 0.717111111]\n",
      "  [0.593508124 0.745964587]\n",
      "  [0.540298343 0.588275075]\n",
      "  ...\n",
      "  [0.570862591 0.49272573]\n",
      "  [0.605455458 0.51120013]\n",
      "  [0.646199644 0.534682333]]\n",
      "\n",
      " [[0.758205891 0.515128255]\n",
      "  [0.871258378 0.373989224]\n",
      "  [0.765470862 0.474690855]\n",
      "  ...\n",
      "  [0.510176957 0.454417288]\n",
      "  [0.557033062 0.500243247]\n",
      "  [0.588110447 0.531782091]]]\n"
     ]
    }
   ],
   "source": [
    "# Version 3: Using softmax\n",
    "tril = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)  # create a banded matrix\n",
    "weights = tf.zeros((T, T), dtype=tf.float32)  # initialize weights\n",
    "weights = tf.where(tril == 0, tf.convert_to_tensor(float(\"-inf\")), tf.convert_to_tensor(0, dtype=tf.float32))  # set weights to 1 where tril is 1\n",
    "weights = tf.nn.softmax(weights, axis=-1)\n",
    "xbow3 = weights @ x  # matrix multiplication, results in shape [B, T, C]\n",
    "print(\"xbow3=\")\n",
    "tf.print(xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.TestCase().assertAllClose(xbow, xbow3)  # check if the two tensors are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (4, 8, 16)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32  # batch size, time, channels\n",
    "x = tf.random.uniform((B, T, C), dtype=tf.float32)  # random input\n",
    "\n",
    "# Single head attention\n",
    "head_size = 16\n",
    "key = keras.layers.Dense(head_size, use_bias=False)  # key projection\n",
    "query = keras.layers.Dense(head_size, use_bias=False)  # query projection\n",
    "value = keras.layers.Dense(head_size, use_bias=False)  # value projection\n",
    "\n",
    "k = key(x)  # shape [B, T, head_size]\n",
    "q = query(x)  # shape [B, T, head_size]\n",
    "v = value(x)  # shape [B, T, head_size]\n",
    "\n",
    "weights = k @ tf.transpose(q, perm=[0, 2, 1])  # shape [B, T, T]\n",
    "\n",
    "tril = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)\n",
    "weights = tf.where(tril == 0, tf.convert_to_tensor(float(\"-inf\")), weights) # This line specifically is only for transformer decoder\n",
    "weights = tf.nn.softmax(weights, axis=-1)\n",
    "\n",
    "output = weights @ v  # shape [B, T, head_size]\n",
    "print(\"output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
